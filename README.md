# Reliability Testing for LLM-Based Systems

As large language models (LLMs) become increasingly integrated into various applications, ensuring their reliability is critical. These systems often take multiple inputs and produce corresponding outputs, each of which must adhere to specific guidelines or criteria. Assessing the reliability of such systems is essential for maintaining trust, safety, and effectiveness. This white paper introduces a framework for conducting reliability tests on LLM-based systems. The framework utilizes validators and verifiers to evaluate the system's behavior across multiple dimensions, providing a comprehensive assessment of its performance.

## Section 1: Key Concepts in Reliability Testing

### Validators: Ensuring Consistent Behavior

Validators are the foundational elements of the reliability testing framework. They are designed to measure how reliably the system adheres to specific instructions or behaviors. For instance, consider a scenario where an LLM is instructed not to use contractions like "isn't," "doesn't," or "can't." A validator can be implemented to assess how well the model follows this rule.

**Example Validator:**

```python
Validator(
    name="contraction_validator",
    message="Output contains too many contractions",
    predicate=lambda o: o.count("'") <= 3,
    minimum_success_percentage=0.95
)
```

Each validator operates on a collection of outputs generated by the system, determining a success percentage that reflects the proportion of outputs meeting the specified criterion.  Sometimes validators can have conditional predicates that rely on the input as well:

**Conditional Validator**

```python
Validator(
    name="politeness_validator",
    message="System seems to have forgotten its manners",
    predicate=lambda i,o: "You're welcome" in o if "Thank you" in i else True,
    success_percentage=0.90
)
```

### Running Binomial Experiments with Validators

Binomial experiments are used to quantify the reliability of the system as determined by a validator. In a Continuous Alignment Testing (CAT) environment, each validator has a minimum success percentage threshold. The outcome of the binomial experiment is compared against this threshold to determine if the system's behavior is reliable.  There are two primary methods for conducting these experiments that can be combined into a third:

### 1.1 **Varying Inputs, Single Output:**

   The system generates a single output for each varied input, and the validator assesses the entire collection of outputs.

   ```text
   input1 -> output11
   input2 -> output12
   input3 -> output13
   ...
   inputN -> output1N
   ```

   ```
   Validator: (output11, output12, ... output1N) -> "success percentage"
   ```

### 1.2 **Fixed Input, Multiple Outputs:**

   A single input is used to generate multiple outputs, and the validator assesses this set of outputs.

   ```text
   inputJ -> output1J, output2J, output3J, ... outputNJ
   ```
   
   ```
   Validator: (output1J, output2J, ... outputNJ) -> "success percentage"
   ```



### 1.3 **Varying Inputs, Multiple Outputs:**

   The natural progression of this pattern leads to a situation where N^2 outputs are generated.  Since generating these outputs takes time, it's important to make the most of them at the time that they're generated, so a careful implementation of a validation strategy is key.  Moreover, as each validation from the Validator is linearly independent, combining the "success percentages" should be done thoughtfully.

   ```text
   input11 -> output11, output21, output31, ... outputN1
   input12 -> output12, output22, output32, ... outputN2
   input11 -> output13, output23, output33, ... outputN3
   ...
   input1N -> output1N, output2N, output3N, ... outputNN
   ```
   
   ```
   Validator: (appropriate subset of outputs) -> "success percentage"
   ```

      - **Validating rows:**
      - **Validating columns:**
      - **Validating all N^2 outputs**


## Section 2: Scaling Reliability Testing with Multiple Validators

In real-world applications, it is often necessary to assess the reliability of a system across multiple dimensions simultaneously. This requires deploying multiple validators, each designed to measure a specific aspect of the system's behavior. In this chapter, we extend the framework discussed in Chapter 1 to accommodate the use of K validators. This approach allows for a more comprehensive evaluation of the system's reliability, as it accounts for the diverse requirements and constraints that a system may need to satisfy.

### 2.1 Understanding the Role of Multiple Validators

When dealing with complex systems, a single validator may not suffice to capture all the nuances of expected behavior. For instance, an LLM may need to meet various criteria such as language style, factual accuracy, ethical considerations, and compliance with specific business rules. Each of these criteria can be represented by a separate validator. The system's overall reliability is then determined by evaluating its performance against all K validators.

**Example Use Case:**

Consider a content generation system where the LLM must adhere to the following rules:

1. **No Contractions:** Avoid using contractions in the output.
2. **Factual Accuracy:** Ensure that all statements are factually correct.
3. **Ethical Compliance:** Avoid generating content that could be considered biased or offensive.
4. **Tone Consistency:** Maintain a consistent, professional tone throughout the output.

Each of these rules would be represented by a separate validator:

```python
Validator(
    name="contraction_validator",
    message="Output contains too many contractions",
    predicate=lambda o: o.count("'") <= 3,
    minimum_success_percentage=0.95
)

Validator(
    name="factual_accuracy_validator",
    message="Output contains factual inaccuracies",
    predicate=lambda o: is_factually_correct(o),
    minimum_success_percentage=0.98
)

Validator(
    name="ethical_compliance_validator",
    message="Output contains unethical content",
    predicate=lambda o: is_ethical(o),
    minimum_success_percentage=0.99
)

Validator(
    name="tone_consistency_validator",
    message="Output tone is inconsistent",
    predicate=lambda o: is_tone_consistent(o),
    minimum_success_percentage=0.97
)
```

### 2.2 Running Binomial Experiments with Multiple Validators

When running binomial experiments with multiple validators, the process can be scaled to evaluate the system's output against each validator independently. The success percentage for each validator is computed based on how well the outputs satisfy the corresponding criterion. The overall reliability of the system is then assessed by combining the results of all validators.

#### 2.2.1 Method 1: Varying Inputs with Multiple Validators

In this method, we vary the inputs, generate outputs for each input, and then apply all K validators to the resulting set of outputs. This approach allows us to assess the system's performance across different scenarios.

**Example:**

```text
input1 -> output11
input2 -> output12
input3 -> output13
...
inputN -> output1N

Validator1: (output11, output12, ... output1N) -> "success percentage"1
Validator2: (output11, output12, ... output1N) -> "success percentage"2
...
ValidatorK: (output11, output12, ... output1N) -> "success percentage"K
```

Each validator produces a success percentage that reflects how reliably the system met the specific criterion it represents across all inputs.

#### 2.2.2 Method 2: Fixed Input with Multiple Validators

In this method, we hold a single input constant and generate multiple outputs for that input. Each validator is then applied to the set of outputs. This method is particularly useful for assessing the consistency of the system’s behavior when responding to a single prompt.

**Example:**

```text
inputK -> output1K, output2K, output3K, ... outputNK

Validator1: (output1K, output2K, ... outputNK) -> "success percentage"1
Validator2: (output1K, output2K, ... outputNK) -> "success percentage"2
...
ValidatorK: (output1K, output2K, ... outputNK) -> "success percentage"K
```

### 2.3 Aggregating Results from Multiple Validators

After obtaining the success percentages from all K validators, the next step is to aggregate these results to form a comprehensive view of the system's reliability. There are several ways to approach this aggregation, depending on the specific requirements of the system and the relative importance of each validator.

#### 2.3.1 Simple Average Method

One straightforward approach is to calculate the simple average of the success percentages across all validators. This method treats each validator equally, providing a general measure of the system’s overall reliability.

```text
Overall Success Percentage = (Success Percentage1 + Success Percentage2 + ... + Success PercentageK) / K
```

#### 2.3.2 Weighted Average Method

In cases where certain behaviors are more critical than others, a weighted average can be used. Each validator is assigned a weight based on its importance, and the overall success percentage is calculated as the weighted sum of the individual success percentages.

```text
Overall Success Percentage = (Weight1 * Success Percentage1 + Weight2 * Success Percentage2 + ... + WeightK * Success PercentageK) / (Weight1 + Weight2 + ... + WeightK)
```

#### 2.3.3 Minimum Threshold Method

Another approach is to set a minimum success threshold that the system must meet across all validators. The overall reliability is then determined by the lowest success percentage recorded among the validators. This method is stringent, ensuring that the system performs reliably across all critical dimensions.

```text
Overall Success Percentage = min(Success Percentage1, Success Percentage2, ..., Success PercentageK)
```

### 2.4 Confidence Intervals with Multiple Validators

As discussed in Chapter 1, confidence intervals provide a range within which the true success percentage likely falls. When dealing with multiple validators, confidence intervals can be calculated for each validator’s success percentage. These intervals can then be reported individually or combined to provide a more nuanced understanding of the system’s reliability.

For each validator, the confidence interval is calculated using the formula:

```text
Confidence Interval for Validator i = Success Percentagei +/- Z * SEi
```

Where `SEi` is the standard error for validator `i`, calculated as:

```text
SEi = sqrt(Success Percentagei * (1 - Success Percentagei) / Ni)
```

The combined confidence interval for the system’s overall reliability can be determined based on the method of aggregation used.

### 2.5 Parallel Execution of Validators

One of the key advantages of using multiple validators is that they can be executed in parallel. This parallelism allows for efficient and scalable testing, particularly in Continuous Alignment Testing (CAT) environments where real-time feedback is crucial.

By running multiple validators simultaneously, the system can quickly identify areas where it meets or falls short of expectations, enabling prompt adjustments and improvements.


## Section 3: Verifier | Assessing System-Wide Reliability

Verifiers provide a holistic assessment of the system's reliability. Unlike validators, which focus on specific aspects of behavior that are possible to evaluate programatically, verifiers evaluate the overall performance of the system. A verifier reviews input-output pairs and determines whether the system's output passes or fails based on a comprehensive set of instructions.

While Validators use predicates , more nuanced validation checks are better suited for the Verification step.  For example, 

**Verifier Process:**

```text
input1 -> output1
input2 -> output2
input3 -> output3
...
inputN -> outputN
```

```
Verifier: (input1,  output1) -> PASS/FAIL
Verifier: (input2,  output2) -> PASS/FAIL
Verifier: (input3,  output3) -> PASS/FAIL
...
Verifier: (inputN,  outputN) -> PASS/FAIL
```

The results of these verification steps are aggregated to assess the overall reliability of the system, but the addition of another call to an LLM in this verification step opens up the possibility for the system to self-correct on a single input-output pair basis.


## 3.1 Verifier Driven Retry Mechanism

Once you have integrated AI into your application, there are several ways to make the system auto-correct.  One of the simplest is to use the verifications step to trigger a "retry."

Since the verifier step uses an LLM transaction to decide whether or not the input-output pair "passes" our test, that same LLM can be made to provide a list of reasons for the failure.  In this case, the input can be augmented with those reasons and sent back through the system to produce another output for the system.  This can be made to repeat MAX times.
 
```
(input, output1) -> Verifier: PASS
    # publish output1

(input, output1) -> Verifier: FAIL, reasons
-> (input+reasons, output2) -> Verifier: PASS
    # publish output2

(input, output1) -> Verifier: FAIL, reasons1
-> (input+reasons1, output2) -> Verifier: FAIL, reasons2
-> (input+reasons2, output3) -> Verifier: FAIL, reasons3
...
-> (input+reasons(MAX-1), outputMAX) -> Verifier: FAIL, reasonsMAX
    # no output published
```

